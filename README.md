# Off-Road Semantic Segmentation with DINOv2


A high-performance semantic segmentation pipeline for off-road terrain imagery. The model leverages a **frozen DINOv2 ViT-S/14** backbone with a custom **Enhanced Multi-Scale Decoder** (ASPP + progressive upsampling) to classify every pixel into one of 10 terrain classes.

All code lives in a single Jupyter notebook: **`main.ipynb`**.

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Directory Structure](#directory-structure)
3. [Dataset Description](#dataset-description)
4. [Class Definitions & Label Encoding](#class-definitions--label-encoding)
5. [Model Architecture](#model-architecture)
6. [Training Strategy](#training-strategy)
7. [Requirements](#requirements)
8. [Reproducing Results — Step by Step](#reproducing-results--step-by-step)
9. [Notebook Cell-by-Cell Walkthrough](#notebook-cell-by-cell-walkthrough)
10. [Output Files](#output-files)
11. [Key Hyperparameters](#key-hyperparameters)
12. [Troubleshooting](#troubleshooting)

---

## Project Overview

| Item | Detail |
|---|---|
| **Task** | Pixel-wise semantic segmentation of off-road scenes |
| **Backbone** | DINOv2 ViT-S/14 (frozen, from `facebookresearch/dinov2`) |
| **Decoder** | Custom Enhanced Segmentation Head with ASPP + ConvTranspose2d progressive upsampling |
| **Loss** | 0.5 × Weighted CrossEntropy (label smoothing 0.05) + 0.5 × Soft Dice |
| **Optimizer** | AdamW (lr=3e-4, weight_decay=1e-4) |
| **Scheduler** | CosineAnnealingWarmRestarts (T_0=10, T_mult=2, eta_min=1e-6) |
| **Precision** | Mixed precision (FP16) via `torch.cuda.amp` |
| **Inference boost** | Test-Time Augmentation (TTA) — original + horizontal flip averaged |
| **Classes** | 10 |
| **Input resolution** | 532 × 952 (multiples of 14, required by DINOv2 patch size) |

---

## Directory Structure

```
Hackethon/
├── main.ipynb                                  # Complete training & evaluation notebook
├── best_segmentation_head.pth                  # Saved best model weights (generated after training)
├── README.md                                   # This file
├── train_segmentation.py                       # Reference training script (not used by notebook)
├── test_segmentation.py                        # Reference testing script (not used by notebook)
├── Offroad_Segmentation_Training_Dataset/
│   ├── train/
│   │   ├── Color_Images/                       # Training RGB images (.png)
│   │   └── Segmentation/                       # Training ground-truth masks (.png, 16-bit)
│   └── val/
│       ├── Color_Images/                       # Validation RGB images (.png)
│       └── Segmentation/                       # Validation ground-truth masks (.png, 16-bit)
├── Offroad_Segmentation_testImages/
│   ├── Color_Images/                           # Test RGB images (.png) — no ground truth
│   ├── Segmentation/                           # Raw predicted masks (generated after inference)
│   └── Segmentation_Colored/                   # Colored predicted masks (generated after inference)
└── ENV_SETUP/                                  # Environment setup files
```

---

## Dataset Description

The dataset contains off-road terrain images captured from various angles and environments. Each image has a corresponding **16-bit grayscale PNG mask** where pixel values encode the terrain class.

| Split | Location | Description |
|---|---|---|
| **Train** | `Offroad_Segmentation_Training_Dataset/train/` | RGB images + ground-truth masks for training |
| **Validation** | `Offroad_Segmentation_Training_Dataset/val/` | RGB images + ground-truth masks for evaluation |
| **Test** | `Offroad_Segmentation_testImages/Color_Images/` | RGB images only — predictions are generated by the model |

- **Image format:** PNG, RGB, varying original resolutions (resized to 532×952 during loading)
- **Mask format:** PNG, 16-bit unsigned integer, where each pixel value maps to a class (see below)
- **File naming:** Images and masks share the **same filename** — e.g., `cc_001.png` in `Color_Images/` corresponds to `cc_001.png` in `Segmentation/`

---

## Class Definitions & Label Encoding

The raw mask files use specific 16-bit pixel values. These are mapped to contiguous class IDs (0–9) during preprocessing:

| Class ID | Class Name | Raw Pixel Value | Color (RGB) | Description |
|:---:|---|:---:|---|---|
| 0 | Background | 0 | (0, 0, 0) Black | Generic background / unlabeled |
| 1 | Trees | 100 | (34, 139, 34) Forest Green | Tree trunks and canopies |
| 2 | Lush Bushes | 200 | (0, 255, 0) Bright Green | Dense green vegetation / bushes |
| 3 | Dry Grass | 300 | (210, 180, 140) Tan | Dried grass and straw |
| 4 | Dry Bushes | 500 | (139, 90, 43) Brown | Dead / dried-out bushes |
| 5 | Ground Clutter | 550 | (128, 128, 0) Olive | Debris, fallen leaves, small objects on ground |
| 6 | Logs | 700 | (139, 69, 19) Saddle Brown | Fallen tree trunks / logs |
| 7 | Rocks | 800 | (128, 128, 128) Gray | Rocks and boulders |
| 8 | Landscape | 7100 | (160, 82, 45) Sienna | Open terrain / landscape features |
| 9 | Sky | 10000 | (135, 206, 235) Sky Blue | Sky regions |

The mapping is implemented in the `convert_mask()` function:
```python
VALUE_MAP = {
    0: 0, 100: 1, 200: 2, 300: 3, 500: 4,
    550: 5, 700: 6, 800: 7, 7100: 8, 10000: 9,
}
```

---

## Model Architecture

### Backbone — DINOv2 ViT-S/14 (Frozen)

- **Model:** `dinov2_vits14` loaded via `torch.hub` from `facebookresearch/dinov2`
- **Embedding dimension:** 384
- **Patch size:** 14×14 pixels
- **Output:** Patch tokens of shape `(B, N_patches, 384)` where `N_patches = (532/14) × (952/14) = 38 × 68 = 2584`
- **All backbone parameters are frozen** — only the decoder is trained
- This provides powerful pre-trained visual features without the cost of fine-tuning a large ViT

### Decoder — EnhancedSegHead

The custom decoder transforms DINOv2 patch tokens into pixel-level class predictions:

```
Patch Tokens (B, 2584, 384)
    │
    ▼ Reshape to spatial grid
Feature Map (B, 384, 38, 68)
    │
    ▼ 1×1 Conv + BN + ReLU
(B, 256, 38, 68)
    │
    ▼ ASPP (Atrous Spatial Pyramid Pooling)
    │   ├── 1×1 conv           → (B, 256, 38, 68)
    │   ├── 3×3 conv, rate=6   → (B, 256, 38, 68)
    │   ├── 3×3 conv, rate=12  → (B, 256, 38, 68)
    │   ├── 3×3 conv, rate=18  → (B, 256, 38, 68)
    │   └── Global AvgPool     → (B, 256, 38, 68)
    │   Concatenate → 1×1 projection + Dropout(0.1)
(B, 256, 38, 68)
    │
    ▼ ConvTranspose2d (×2 upsample) + 2× ConvBNReLU
(B, 128, 76, 136)
    │
    ▼ ConvTranspose2d (×2 upsample) + 2× ConvBNReLU
(B, 64, 152, 272)
    │
    ▼ ConvBNReLU + Dropout(0.1) + 1×1 Conv
(B, 10, 152, 272)
    │
    ▼ Bilinear Interpolation to original size
(B, 10, 532, 952) — per-pixel logits for 10 classes
```

**Key modules:**
- **`ConvBNReLU`** — A reusable block: Conv2d → BatchNorm2d → ReLU
- **`ASPP`** — Atrous Spatial Pyramid Pooling captures multi-scale context using dilated convolutions at rates 6, 12, 18 plus a global average pooling branch
- **`EnhancedSegHead`** — The full decoder combining projection, ASPP, progressive upsampling, and classification

---

## Training Strategy

### Data Augmentation (Albumentations)

**Training augmentations** (applied on-the-fly):
| Augmentation | Parameters | Probability |
|---|---|---|
| Resize | 532 × 952 | Always |
| HorizontalFlip | — | 50% |
| VerticalFlip | — | 10% |
| RandomBrightnessContrast | brightness ±0.2, contrast ±0.2 | 50% |
| HueSaturationValue | hue ±10, sat ±20, val ±20 | 30% |
| GaussNoise | variance 10–50 | 20% |
| GaussianBlur | kernel 3–5 | 20% |
| RandomResizedCrop | scale 0.5–1.0, ratio 0.8–1.2 | 30% |
| Normalize | ImageNet mean/std | Always |

**Validation augmentations:** Resize + Normalize only (no randomness).

### Class Balancing

Class weights are computed from a random sample of 200 training images using **inverse frequency weighting**:
1. Count pixels per class across the sample
2. Compute `weight = 1 / (frequency + epsilon)`
3. Normalize weights so the mean equals 1
4. Clip to range [0.3, 5.0] to prevent extreme values

These weights are passed to the CrossEntropy loss component.

### Loss Function — `CombinedLoss`

```
Total Loss = 0.5 × Weighted CrossEntropy + 0.5 × Soft Dice Loss
```

- **Weighted CrossEntropy:** Uses computed class weights + label smoothing (0.05) to improve generalization
- **Soft Dice Loss:** Directly optimizes the overlap between predicted and ground-truth regions, which correlates well with IoU

### Optimization

- **Optimizer:** AdamW with learning rate 3×10⁻⁴ and weight decay 1×10⁻⁴
- **Scheduler:** CosineAnnealingWarmRestarts (T_0=10, T_mult=2, eta_min=1×10⁻⁶) — provides periodic warm restarts that help escape local minima
- **Mixed precision:** GradScaler + autocast for ~2× training speed on NVIDIA GPUs
- **Gradient clipping:** max_norm=1.0 to stabilize training
- **Early stopping:** Training halts if validation mIoU doesn't improve for 8 consecutive epochs

### Test-Time Augmentation (TTA)

At inference, the model averages softmax predictions from:
1. The original image
2. A horizontally flipped version (flipped back before averaging)

This typically provides a small but consistent mIoU improvement.

---

## Requirements

### Hardware
- **GPU:** NVIDIA GPU with at least 6 GB VRAM (tested; CUDA required for practical training speed)
- **RAM:** 8+ GB system RAM
- **Disk:** ~2 GB for dataset + model weights

### Software
- Python 3.8+
- CUDA 11.x or 12.x (matching your GPU driver)

### Python Packages

```
torch >= 2.0
torchvision >= 0.15
numpy
matplotlib
scikit-learn
opencv-python
Pillow
tqdm
albumentations >= 1.3
segmentation-models-pytorch
```

---

## Reproducing Results — Step by Step

### 1. Clone / Download the Project

Ensure the following structure exists:
```
Hackethon/
├── main.ipynb
├── Offroad_Segmentation_Training_Dataset/
│   ├── train/
│   │   ├── Color_Images/
│   │   └── Segmentation/
│   └── val/
│       ├── Color_Images/
│       └── Segmentation/
└── Offroad_Segmentation_testImages/
    └── Color_Images/
```

### 2. Install Dependencies

Open a terminal in the `Hackethon/` directory and run:
```bash
pip install torch torchvision
pip install numpy matplotlib scikit-learn opencv-python Pillow tqdm albumentations segmentation-models-pytorch
```

Or simply run **Cell 1** of the notebook, which executes these same install commands.

### 3. Open the Notebook

Open `main.ipynb` in Jupyter Notebook, JupyterLab, or VS Code with the Jupyter extension.

### 4. Run All Cells Sequentially

Execute every cell from top to bottom (Cell 1 through Cell 19). The notebook is designed to run end-to-end:

| Step | Cell(s) | What Happens | Time Estimate |
|:---:|:---:|---|---|
| Install | 1 | Installs all pip packages | 1–3 min |
| Imports | 3 | Loads all libraries, detects GPU | Instant |
| Config | 4 | Sets paths, class mapping, hyperparameters | Instant |
| Dataset | 5 | Creates train/val datasets and DataLoaders | ~10 sec |
| Visualize | 6 | Displays 3 sample images with masks | ~5 sec |
| Class Weights | 7 | Samples 200 images, computes inverse-frequency weights | ~1 min |
| Backbone | 8 | Downloads and loads DINOv2 ViT-S/14 (first run downloads ~85 MB) | 30 sec – 2 min |
| Decoder | 9 | Instantiates the EnhancedSegHead decoder | Instant |
| Loss | 10 | Creates CombinedLoss (Dice + CE) | Instant |
| Optimizer | 11 | Sets up AdamW + CosineAnnealingWarmRestarts + GradScaler | Instant |
| Metrics | 12 | Defines compute_iou, compute_pixel_accuracy, evaluate functions | Instant |
| **Training** | **13** | **Trains for up to 30 epochs with early stopping** | **30 min – 2 hr** (GPU-dependent) |
| Curves | 14 | Plots loss, mIoU, accuracy, and LR curves | Instant |
| Eval | 15 | Loads best model, plots per-class IoU bar chart, prints final metrics | ~1 min |
| Vis Preds | 16 | Shows 6 validation predictions side-by-side with ground truth | ~30 sec |
| TTA | 17 | Evaluates with Test-Time Augmentation on full val set | ~5 min |
| Test Preds | 18 | Generates raw 16-bit mask predictions for test images using TTA | ~5 min |
| Colored Preds | 19 | Saves colored visualizations of test predictions + displays samples | ~1 min |

### 5. Collect Results

After training completes:
- **Best model weights** → `best_segmentation_head.pth`
- **Raw test predictions** (16-bit PNG) → `Offroad_Segmentation_testImages/Segmentation/`
- **Colored test predictions** (RGB PNG) → `Offroad_Segmentation_testImages/Segmentation_Colored/`
- **Metrics** are printed in the notebook output (mIoU, pixel accuracy, per-class IoU)
- **Plots** are displayed inline (training curves, per-class IoU bar chart, prediction visualizations)

---

## Notebook Cell-by-Cell Walkthrough

### Cell 1 — Package Installation
Runs `pip install` for all required packages. Only needs to run once per environment.

### Cell 2 — Markdown Header
Overview of the approach, listing backbone, decoder, loss, and augmentation choices.

### Cell 3 — Imports & Device Detection
Imports all libraries (PyTorch, OpenCV, Albumentations, etc.) and auto-detects CUDA GPU. Prints GPU name and available VRAM.

### Cell 4 — Configuration
Defines all constants in one place:
- **Paths:** `BASE_DIR`, `TRAIN_DIR`, `VAL_DIR` — relative to the notebook's directory
- **Class mapping:** `VALUE_MAP` maps raw 16-bit pixel values → class IDs 0–9
- **Class names & colors:** `CLASS_NAMES` and `COLOR_PALETTE` for visualization
- **Hyperparameters:** Image size (532×952), batch size (4), learning rate (3e-4), epochs (30), patience (8), backbone size ("small")

### Cell 5 — Dataset & DataLoaders
- **`convert_mask()`** — Takes a raw 16-bit mask array and maps every pixel value to a class ID (0–9) using `VALUE_MAP`
- **Training augmentation pipeline** — Albumentations Compose with geometric transforms (flip, crop), photometric transforms (brightness, hue, noise, blur), resize, and ImageNet normalization
- **Validation pipeline** — Resize + Normalize only
- **`OffroadSegDataset`** — Custom PyTorch Dataset that reads image-mask pairs, applies augmentation, and returns `(image_tensor, mask_tensor)`
- **DataLoaders** — Train loader with shuffle + drop_last; val loader without shuffle. Both use `num_workers=2` and `pin_memory=True`

### Cell 6 — Sample Visualization
Displays 3 training samples showing: original image, color-coded ground truth mask, and raw class ID mask. Uses `denorm()` to reverse ImageNet normalization and `mask_to_color()` to convert class IDs to RGB.

### Cell 7 — Class Weight Computation
Randomly samples 200 training images and counts pixels per class. Computes inverse-frequency weights, normalizes them (mean=1), and clips to [0.3, 5.0]. Prints the distribution table and moves weights to GPU.

### Cell 8 — DINOv2 Backbone Loading
Downloads `dinov2_vits14` from `facebookresearch/dinov2` via `torch.hub`. Freezes all parameters. Runs a dummy forward pass to determine embedding dimension (384), patch grid size (38×68), and total patch count (2584).

### Cell 9 — Enhanced Segmentation Decoder
Defines three modules:
- **`ConvBNReLU`** — Conv2d + BatchNorm + ReLU building block
- **`ASPP`** — Multi-scale feature extraction with 1×1 conv, three dilated convs (rates 6, 12, 18), and global average pooling. All branches are concatenated and projected down via 1×1 conv.
- **`EnhancedSegHead`** — Full decoder: reshapes flat patch tokens to a 2D feature map, projects to 256 channels, applies ASPP, then progressively upsamples via two ConvTranspose2d stages (256→128→64), and classifies with a 1×1 conv.

Prints total trainable parameter count.

### Cell 10 — Combined Loss Function
- **`DiceLoss`** — Computes soft Dice loss across all classes using one-hot encoding and smooth factor
- **`CombinedLoss`** — 50/50 blend of weighted CrossEntropy (with label smoothing 0.05) and Dice loss

### Cell 11 — Optimizer & Scheduler
Creates AdamW optimizer (only for `seg_head` parameters), CosineAnnealingWarmRestarts scheduler, and GradScaler for mixed precision.

### Cell 12 — Metric Functions
- **`compute_iou()`** — Per-class Intersection-over-Union from logits and targets. Returns NaN for absent classes.
- **`compute_pixel_accuracy()`** — Fraction of correctly classified pixels.
- **`evaluate()`** — Runs the full model through a DataLoader, computes average loss, per-class IoU, mIoU, and pixel accuracy. Uses mixed precision.

### Cell 13 — Training Loop
The main training loop:
1. For each epoch, iterates over training batches
2. Extracts frozen DINOv2 features (`torch.no_grad()`)
3. Passes features through the decoder
4. Bilinear-interpolates logits to original image size
5. Computes combined loss, backpropagates with mixed precision
6. Clips gradients (max_norm=1.0)
7. At epoch end: runs full validation evaluation
8. Saves model if validation mIoU improves (`best_segmentation_head.pth`)
9. Early stopping if no improvement for 8 epochs
10. Prints per-epoch summary with per-class IoU breakdown

### Cell 14 — Training Curves
Plots 4 subplots: train/val loss, train/val mIoU (with best marker), pixel accuracy, and learning rate schedule.

### Cell 15 — Per-Class IoU Bar Chart & Final Evaluation
Loads the best saved model, evaluates on the full validation set, and displays a bar chart of per-class IoU with the mean IoU line. Prints a formatted results table.

### Cell 16 — Validation Prediction Visualization
Shows 6 evenly-spaced validation images with ground truth and model prediction side-by-side. Each prediction is annotated with its per-image mIoU.

### Cell 17 — Test-Time Augmentation (TTA) Evaluation
Evaluates the best model on every validation image using TTA (original + horizontal flip averaged). Reports the TTA mIoU and improvement over non-TTA.

### Cell 18 — Test Set Prediction
For each test image:
1. Loads and preprocesses with validation augmentation
2. Predicts using TTA for best quality
3. Resizes prediction back to original resolution (nearest-neighbor)
4. Converts class IDs back to raw 16-bit pixel values using `REVERSE_MAP`
5. Saves as 16-bit PNG to `Offroad_Segmentation_testImages/Segmentation/`

### Cell 19 — Colored Predictions & Visualization
Reads the raw predictions from Cell 18, converts them to colored RGB images using the class color palette, and saves to `Segmentation_Colored/`. Displays 6 sample test images with their colored predictions and a class color legend.

---

## Output Files

| File / Directory | Description |
|---|---|
| `best_segmentation_head.pth` | PyTorch state dict of the best decoder (by val mIoU) |
| `Offroad_Segmentation_testImages/Segmentation/*.png` | Raw 16-bit test masks (pixel values match original encoding) |
| `Offroad_Segmentation_testImages/Segmentation_Colored/*.png` | RGB colored test masks for visual inspection |

---

## Key Hyperparameters

All hyperparameters are defined in **Cell 4** and can be adjusted there:

| Parameter | Default | Notes |
|---|---|---|
| `IMG_H` / `IMG_W` | 532 / 952 | Must be multiples of 14 (DINOv2 patch size). Nearest multiples of 540/960. |
| `BATCH_SIZE` | 4 | Reduce to 2 if you get CUDA OOM errors |
| `LR` | 3e-4 | Initial learning rate for AdamW |
| `WEIGHT_DECAY` | 1e-4 | L2 regularization strength |
| `N_EPOCHS` | 30 | Maximum training epochs (may stop early) |
| `PATIENCE` | 8 | Early stopping patience (epochs without mIoU improvement) |
| `BACKBONE_SIZE` | "small" | Options: `"small"` (ViT-S, 384d), `"base"` (ViT-B, 768d), `"large"` (ViT-L, 1024d) |

---

## Troubleshooting

| Problem | Solution |
|---|---|
| **CUDA out of memory** | Reduce `BATCH_SIZE` to 2 or 1 in Cell 4. Or reduce image size (e.g., `IMG_H=378`, `IMG_W=672`). |
| **`num_workers` errors on Windows** | Change `num_workers=2` to `num_workers=0` in Cell 5 DataLoader calls. |
| **DINOv2 download fails** | Ensure internet access. The model is downloaded once to `~/.cache/torch/hub/`. |
| **Albumentations API errors** | Ensure `albumentations >= 1.3`. The notebook uses `size=(H, W)` tuple syntax for `RandomResizedCrop`. |
| **Path not found** | The notebook must be run from inside the `Hackethon/` directory. All paths are relative to it. |
| **Test directory not found** | Cell 18 will print a warning and skip. Ensure `Offroad_Segmentation_testImages/Color_Images/` exists with `.png` files. |
| **Low IoU on specific classes** | Rare classes (Logs, Ground Clutter) naturally have lower IoU. You can increase their class weight cap in Cell 7 or add targeted augmentation. |

---

## License

This project was developed for hackathon/research purposes. The DINOv2 backbone is provided by Meta AI under the Apache 2.0 license.